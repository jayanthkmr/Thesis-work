{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import gc\n",
    "import resource\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 train samples, 32 channels, 32x3\n",
      "10000  test samples, 32 channels, 32x3\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "num_classes = 100\n",
    "nb_classes = 20\n",
    "epochs = 50\n",
    "learning_rate = 3e-4\n",
    "from keras.datasets import cifar100\n",
    "from keras.utils import np_utils\n",
    "(X_train, y_train), (X_test, y_test) = cifar100.load_data('coarse')\n",
    "(Xf_train, yf_train), (Xf_test, yf_test) = cifar100.load_data('fine')\n",
    "\n",
    "# print shape of data while model is building\n",
    "print(\"{1} train samples, {2} channel{0}, {3}x{4}\".format(\"\" if X_train.shape[1] == 1 else \"s\", *X_train.shape))\n",
    "print(\"{1}  test samples, {2} channel{0}, {3}x{4}\".format(\"\" if X_test.shape[1] == 1 else \"s\", *X_test.shape))\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Yf_train = np_utils.to_categorical(yf_train, num_classes)\n",
    "Yf_test = np_utils.to_categorical(yf_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n",
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.Size([50000])\n",
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "tensor_x_train = torch.Tensor(X_train).permute(0,3,1,2)\n",
    "tensor_x_test = torch.Tensor(X_test).permute(0,3,1,2)\n",
    "tensor_yc_train = torch.LongTensor(y_train.flatten())\n",
    "tensor_yf_train = torch.LongTensor(yf_train.flatten())\n",
    "print(tensor_x_train.shape)\n",
    "print(tensor_x_test.shape)\n",
    "print(tensor_yc_train.shape)\n",
    "print(tensor_yf_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.Size([10000])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "tensor_yc_test = torch.LongTensor(y_test.flatten())\n",
    "tensor_yf_test = torch.LongTensor(yf_test.flatten())\n",
    "print(tensor_x_test.shape)\n",
    "print(tensor_yc_test.shape)\n",
    "print(tensor_yf_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(tensor_x_train,tensor_yc_train,tensor_yf_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(tensor_x_test,tensor_yc_test,tensor_yf_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Implements Adam algorithm.\n",
    "\n",
    "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "    \n",
    "    def step(self, alpha, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = alpha * group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([200, 400])\n",
      "torch.Size([200])\n",
      "torch.Size([20, 200])\n",
      "torch.Size([20])\n",
      "torch.Size([100, 200])\n",
      "torch.Size([100])\n",
      "Parameter containing:\n",
      "tensor([ 0.0526, -0.0428,  0.0531,  0.0336, -0.0322,  0.0150,  0.0073, -0.0378,\n",
      "         0.0040,  0.0243,  0.0439,  0.0507, -0.0207,  0.0406,  0.0664, -0.0478,\n",
      "        -0.0358,  0.0276, -0.0467,  0.0472, -0.0515,  0.0094, -0.0120, -0.0611,\n",
      "        -0.0462, -0.0122, -0.0690,  0.0613, -0.0282, -0.0258,  0.0550,  0.0472,\n",
      "        -0.0312,  0.0004, -0.0641,  0.0118, -0.0403, -0.0209,  0.0581,  0.0508,\n",
      "         0.0347, -0.0338,  0.0431,  0.0335, -0.0251,  0.0421, -0.0617, -0.0623,\n",
      "         0.0629, -0.0391, -0.0622,  0.0078, -0.0629,  0.0060, -0.0076, -0.0603,\n",
      "         0.0556,  0.0177,  0.0035, -0.0297, -0.0376,  0.0263,  0.0226,  0.0335,\n",
      "         0.0094,  0.0078,  0.0267, -0.0573, -0.0184, -0.0239,  0.0336, -0.0572,\n",
      "        -0.0272,  0.0687,  0.0465, -0.0338, -0.0270,  0.0109, -0.0630,  0.0374,\n",
      "        -0.0364, -0.0245, -0.0186,  0.0092,  0.0001,  0.0285, -0.0515,  0.0627,\n",
      "        -0.0660, -0.0099,  0.0439,  0.0424, -0.0330, -0.0326,  0.0670, -0.0094,\n",
      "        -0.0450, -0.0139,  0.0548, -0.0004], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) #28*28*6\n",
    "        self.pool = nn.MaxPool2d(2, 2) #14*14*6\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) #5*5*16\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 200)\n",
    "        self.fc2 = nn.Linear(200, 20)\n",
    "        self.fc3 = nn.Linear(200, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x)) #16 * 5 * 5\n",
    "        x = F.relu(self.fc1(x))\n",
    "        y = self.fc2(x)\n",
    "        z = self.fc3(x)\n",
    "        return y,z\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "for i in net.parameters():\n",
    "    print(i.shape)\n",
    "    if i.shape == torch.Size([100]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayanth/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:33: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0023)\n",
      "tensor(0.0023)\n",
      "tensor(0.0023)\n",
      "alpha 0.500\n",
      "tensor(0.0020)\n",
      "tensor(0.0020)\n",
      "tensor(0.0020)\n",
      "alpha 0.500\n",
      "tensor(0.0020)\n",
      "tensor(0.0020)\n",
      "tensor(0.0020)\n",
      "alpha 0.500\n",
      "tensor(0.0019)\n",
      "tensor(0.0019)\n",
      "tensor(0.0019)\n",
      "alpha 0.500\n",
      "tensor(0.0019)\n",
      "tensor(0.0019)\n",
      "tensor(0.0019)\n",
      "alpha 0.500\n",
      "tensor(0.0018)\n",
      "tensor(0.0018)\n",
      "tensor(0.0018)\n",
      "alpha 0.500\n",
      "tensor(0.0018)\n",
      "tensor(0.0018)\n",
      "tensor(0.0018)\n",
      "alpha 0.500\n",
      "tensor(0.0017)\n",
      "tensor(0.0017)\n",
      "tensor(0.0017)\n",
      "alpha 0.500\n",
      "tensor(0.0017)\n",
      "tensor(0.0017)\n",
      "tensor(0.0017)\n",
      "alpha 0.500\n",
      "tensor(0.0017)\n",
      "tensor(0.0017)\n",
      "tensor(0.0017)\n",
      "alpha 0.500\n",
      "tensor(0.0016)\n",
      "tensor(0.0016)\n",
      "tensor(0.0016)\n",
      "alpha 0.500\n",
      "tensor(0.0016)\n",
      "tensor(0.0016)\n",
      "tensor(0.0016)\n",
      "alpha 0.500\n",
      "tensor(0.0015)\n",
      "tensor(0.0015)\n",
      "tensor(0.0015)\n",
      "alpha 0.500\n",
      "tensor(0.0015)\n",
      "tensor(0.0015)\n",
      "tensor(0.0015)\n",
      "alpha 0.500\n",
      "tensor(0.0014)\n",
      "tensor(0.0014)\n",
      "tensor(0.0014)\n",
      "alpha 0.500\n",
      "tensor(0.0014)\n",
      "tensor(0.0014)\n",
      "tensor(0.0014)\n",
      "alpha 0.500\n",
      "tensor(0.0013)\n",
      "tensor(0.0013)\n",
      "tensor(0.0013)\n",
      "alpha 0.500\n",
      "tensor(0.0013)\n",
      "tensor(0.0013)\n",
      "tensor(0.0013)\n",
      "alpha 0.500\n",
      "tensor(0.0013)\n",
      "tensor(0.0013)\n",
      "tensor(0.0013)\n",
      "alpha 0.500\n",
      "tensor(0.0013)\n",
      "tensor(0.0013)\n",
      "tensor(0.0013)\n",
      "alpha 0.500\n",
      "Epoch [1/50], Iter [20/0] Loss: 3.2387, 4.7713, 4.0050\n",
      "tensor(0.0012)\n",
      "tensor(0.0012)\n",
      "tensor(0.0012)\n",
      "alpha 0.500\n",
      "tensor(0.0011)\n",
      "tensor(0.0011)\n",
      "tensor(0.0011)\n",
      "alpha 0.500\n",
      "tensor(0.0011)\n",
      "tensor(0.0011)\n",
      "tensor(0.0011)\n",
      "alpha 0.500\n",
      "tensor(0.0011)\n",
      "tensor(0.0011)\n",
      "tensor(0.0011)\n",
      "alpha 0.500\n",
      "tensor(0.0010)\n",
      "tensor(0.0010)\n",
      "tensor(0.0010)\n",
      "alpha 0.500\n",
      "tensor(0.0010)\n",
      "tensor(0.0010)\n",
      "tensor(0.0010)\n",
      "alpha 0.500\n",
      "tensor(0.0010)\n",
      "tensor(0.0010)\n",
      "tensor(0.0010)\n",
      "alpha 0.500\n",
      "tensor(0.0009)\n",
      "tensor(0.0009)\n",
      "tensor(0.0009)\n",
      "alpha 0.500\n",
      "tensor(0.0009)\n",
      "tensor(0.0009)\n",
      "tensor(0.0009)\n",
      "alpha 0.500\n",
      "tensor(0.0009)\n",
      "tensor(0.0009)\n",
      "tensor(0.0009)\n",
      "alpha 0.500\n",
      "tensor(0.0008)\n",
      "tensor(0.0008)\n",
      "tensor(0.0008)\n",
      "alpha 0.500\n",
      "tensor(0.0008)\n",
      "tensor(0.0008)\n",
      "tensor(0.0008)\n",
      "alpha 0.500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fca04ca4e77b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mlossf_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mlossgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jayanth/Library/Python/2.7/lib/python/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jayanth/Library/Python/2.7/lib/python/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Loss and Optimizer\n",
    "\n",
    "#Can also try nn.MSELoss()\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer1 = Adam([i for i in net.parameters() if i.shape != torch.Size([100]) and i.shape != torch.Size([100, 200])], lr=learning_rate, weight_decay=0.03)\n",
    "optimizer2 = Adam([i for i in net.parameters() if i.shape != torch.Size([20]) and i.shape != torch.Size([20, 200])], lr=learning_rate, weight_decay=0.03)\n",
    "\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "trainc_acc = []\n",
    "valc_acc = []\n",
    "trainf_acc = []\n",
    "valf_acc = []\n",
    "\n",
    "loss_history = []\n",
    "lossc_history = []\n",
    "lossf_history = []\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labelc, labelf) in enumerate(train_loader):\n",
    "        net.train() # Change model to 'train' mode.\n",
    "    #     for i in range(cnn_training_data_X.shape[0]):\n",
    "        images = Variable(images, requires_grad=True) #unsqueeze used to make a 4d tensor because \n",
    "    #     print images.shape\n",
    "        labelc = Variable(labelc, requires_grad=False)\n",
    "        labelf = Variable(labelf, volatile=True)\n",
    "        #labels = [labelc, labelf]\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        net.zero_grad()\n",
    "        #outputs = net(images)\n",
    "        \n",
    "        \n",
    "        outc, outf = net(images)\n",
    "        lossc = criterion1(outc, labelc)\n",
    "        lossf = criterion2(outf, labelf)\n",
    "        \n",
    "        #if alpha is None:\n",
    "        #    alpha = lossf/(lossc+lossf)\n",
    "            \n",
    "        loss = alpha*lossc + (1-alpha)*lossf\n",
    "        \n",
    "        #loss = criterion(outputs, labelc)\n",
    "        #loss.backward()\n",
    "        \n",
    "        loss_history.append(loss.data)\n",
    "        lossc_history.append(lossc.data)\n",
    "        lossf_history.append(lossf.data)\n",
    "        \n",
    "        loss.backward()\n",
    "        lossgrad = images.grad\n",
    "        \n",
    "        losscgrad = (1-alpha)*lossgrad/alpha\n",
    "        lossfgrad = alpha*lossgrad/(1-alpha)\n",
    "        \n",
    "        losscgradnorm = torch.norm(torch.norm(torch.norm(torch.norm(losscgrad, 2, 0),2,0),2,0),2,0)\n",
    "        lossfgradnorm = torch.norm(torch.norm(torch.norm(torch.norm(lossfgrad, 2, 0),2,0),2,0),2,0)\n",
    "        alpha = (lossfgradnorm**2)/((lossfgradnorm**2) + (losscgradnorm**2))\n",
    "        \n",
    "        \n",
    "        print(losscgradnorm)\n",
    "        print(lossfgradnorm)\n",
    "        print(torch.norm(torch.norm(torch.norm(torch.norm(lossgrad, 2, 0),2,0),2,0),2,0))\n",
    "        print('alpha %.3f'%(alpha))\n",
    "        \n",
    "        optimizer1.step(alpha=alpha.data)\n",
    "        optimizer2.step(alpha=(1-alpha.data))\n",
    "\n",
    "        #optimizer.step()\n",
    "        if (i+1) % 20 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f, %.4f, %.4f' \n",
    "                   %(epoch+1, epochs, i+1, len(X_train.shape)//batch_size, lossc.data, lossf.data,loss.data))\n",
    "    print('[%d/%d] Loss: %.3f' % (epoch+1, epochs, np.mean(loss_history)))\n",
    "    \n",
    "    \n",
    "    correctc = 0\n",
    "    correctf = 0\n",
    "    total = 0\n",
    "    net.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "    with torch.no_grad():\n",
    "        for data in train_loader:\n",
    "            images, labelc, labelf = data\n",
    "            outputc, outputf = net(images)\n",
    "            _, predictedc = torch.max(outputc.data, 1)\n",
    "            _, predictedf = torch.max(outputf.data, 1)\n",
    "            total += labelc.size(0)\n",
    "            correctc += (predictedc == labelc).sum().item()\n",
    "            correctf += (predictedf == labelf).sum().item()\n",
    "    print(total)\n",
    "    trainc_acc_val = (100 * correctc / total)\n",
    "    trainf_acc_val = (100 * correctf / total)\n",
    "    train_acc_val = (trainc_acc_val + trainf_acc_val)/2.0\n",
    "    trainc_acc.append(trainc_acc_val)\n",
    "    trainf_acc.append(trainf_acc_val)\n",
    "    train_acc.append(train_acc_val)\n",
    "    print('Accuracy of the network on the 50000 test images coarse: %d %%' % (trainc_acc_val))\n",
    "    print('Accuracy of the network on the 50000 test images fine: %d %%' % (trainf_acc_val))\n",
    "    print('Average Accuracy of the network on the 50000 test images: %d %%' % (train_acc_val))\n",
    "          \n",
    "    correctc = 0\n",
    "    correctf = 0\n",
    "    total = 0\n",
    "    net.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labelc, labelf = data\n",
    "            outputc, outputf = net(images)\n",
    "            _, predictedc = torch.max(outputc.data, 1)\n",
    "            _, predictedf = torch.max(outputf.data, 1)\n",
    "            total += labelc.size(0)\n",
    "            correctc += (predictedc == labelc).sum().item()\n",
    "            correctf += (predictedf == labelf).sum().item()\n",
    "\n",
    "    valc_acc_val = (100 * correctc / total)\n",
    "    valf_acc_val = (100 * correctf / total)\n",
    "    val_acc_val = (valc_acc_val + valf_acc_val)/2.0\n",
    "    valc_acc.append(valc_acc_val)\n",
    "    valf_acc.append(valf_acc_val)\n",
    "    val_acc.append(val_acc_val)\n",
    "    print('Accuracy of the network on the 50000 test images coarse: %d %%' % (valc_acc_val))\n",
    "    print('Accuracy of the network on the 50000 test images fine: %d %%' % (valf_acc_val))\n",
    "    print('Average Accuracy of the network on the 50000 test images: %d %%' % (val_acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35.5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot train/val accuracies\n",
    "print(train_acc)\n",
    "plt.title(\"Train/Test Acc\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel('acc')\n",
    "plt.plot(train_acc, color='red')\n",
    "plt.plot(val_acc, color='blue')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
